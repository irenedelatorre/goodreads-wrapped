# GENRES (works for new + fallbacks)
genres <- html %>%
html_elements("[data-testid='genresList'] .Button__labelItem") %>%
trim_vec()
if (!length(genres)) {
genres <- html %>%
html_elements(
".BookPageMetadataSection__genreButton .Button__labelItem, \
.BookPageMetadataSection__genres a.Button--tag .Button__labelItem, \
a.bookPageGenreLink, \
.bookPageGenres a"
) %>% trim_vec()
}
# --- inside extract_from_doc(html), replace the LANGUAGE section with this ---
language <- NA_character_
# Labels in several locales (extend if you see others)
lang_labels <- c("Language", "Idioma", "LÃ­ngua", "Langue", "Sprache", "Lingua")
# 1) New layout: <div class="EditionDetails"> ... <dl class="DescList">
#    <div class="DescListItem"><dt>Language</dt><dd> ... contentContainer ... </dd></div>
lang_xpath_item <- paste0(
"//div[contains(@class,'EditionDetails')]",
"//dl[contains(@class,'DescList')]",
"//div[contains(@class,'DescListItem')][.//dt[normalize-space() = '",
paste(lang_labels, collapse = "' or normalize-space() = '"),
"']]"
)
item <- rvest::html_element(html, xpath = lang_xpath_item)
if (!is.null(item)) {
# prefer explicit content container inside dd; fall back to any text in dd
lang_nodes <- rvest::html_elements(
item,
xpath = ".//dd//*[(@data-testid='contentContainer') or contains(@class,'TruncatedContent__text')]"
)
if (length(lang_nodes) == 0) {
lang_nodes <- rvest::html_elements(item, xpath = ".//dd")
}
lang_txt <- lang_nodes |>
rvest::html_text2() |>
stringr::str_squish() |>
clean_invisibles()
lang_txt <- lang_txt[nzchar(lang_txt)]
if (length(lang_txt) > 0) language <- lang_txt[1]
}
# 2) Generic dtâ†’dd pairing anywhere (fallback)
if (is.na(language) || !nzchar(language)) {
dts <- rvest::html_elements(html, xpath = "//dt")
if (length(dts) > 0) {
for (dt in dts) {
lbl <- dt |> rvest::html_text2() |> stringr::str_squish() |> clean_invisibles()
if (tolower(lbl) %in% tolower(lang_labels)) {
dd <- rvest::html_element(dt, xpath = "following-sibling::dd[1]")
if (!is.null(dd)) {
val <- dd |>
rvest::html_text2() |>
stringr::str_squish() |>
clean_invisibles()
if (nzchar(val)) { language <- val; break }
}
}
}
}
}
# 3) Legacy fallback (old BookDataBox/details)
if (is.na(language) || !nzchar(language)) {
legacy_nodes <- rvest::html_elements(
html,
xpath = "//*[@id='bookDataBox' or @id='details' or contains(@class,'BookDataBox')]"
)
if (length(legacy_nodes) > 0) {
legacy_txt <- legacy_nodes |>
rvest::html_text2() |>
paste(collapse = "  ") |>
clean_invisibles()
m <- stringr::str_match(
legacy_txt,
"(?i)\\b(Edition\\s+Language|Language)\\s*:?\\s*([A-Za-zÃ€-Ã¿ ;,\\-]+)"
)
if (!all(is.na(m))) {
candidate <- m[, ncol(m)] |> stringr::str_squish() |> clean_invisibles()
if (nzchar(candidate)) language <- candidate
}
}
}
# ensure scalar or NA
list(
genres_vec = unique(genres[nzchar(genres)]),
genres     = uniq_collapse(genres),
language = if (is.na(language) || !nzchar(language)) NA_character_ else language
)
}
# FAST PATH
fast_ok <- FALSE
res <- list(genres_vec = character(), genres = NA_character_, language = NA_character_)
try({
html_fast <- xml2::read_html(book_url)
res_fast  <- extract_from_doc(html_fast)
if (length(res_fast$genres_vec) > 0 || !is.na(res_fast$language)) {
res <- res_fast
fast_ok <- TRUE
}
}, silent = TRUE)
if (fast_ok) return(res)
# FALLBACK: Selenium
retry_nav(book_url)
try(remDr$executeScript("window.scrollTo(0, document.body.scrollHeight);"),
silent = TRUE)
t0 <- Sys.time(); html_slow <- NULL
repeat {
Sys.sleep(0.7)
src <- remDr$getPageSource()[[1]]
if (!length(src)) next
html_slow <- xml2::read_html(src)
has_genres <- length(html_elements(
html_slow,
"[data-testid='genresList'], .BookPageMetadataSection__genres, \
a.bookPageGenreLink, .bookPageGenres a"
)) > 0
has_details <- length(html_elements(html_slow, ".EditionDetails, dl.DescList")) > 0
has_legacy  <- length(html_elements(html_slow, "#bookDataBox, #details")) > 0
if (has_genres || has_details || has_legacy) break
if (as.numeric(difftime(Sys.time(), t0, units = "secs")) > wait_timeout) break
}
if (is.null(html_slow)) {
return(list(genres_vec = character(), genres = NA_character_, language = NA_character_))
}
extract_from_doc(html_slow)
}
test_book_metadata <- function(book_url, save_html = TRUE) {
library(xml2); library(rvest)
cat("\n=== Testing:", book_url, "===\n")
# 1) Try fast path
ok_fast <- FALSE
html_fast <- NULL
try({
html_fast <- read_html(book_url)
g_fast <- html_elements(html_fast, "[data-testid='genresList'] .Button__labelItem")
d_fast <- html_elements(html_fast, ".EditionDetails .DescListItem")
cat("Fast path: genres nodes =", length(g_fast),
"| edition items =", length(d_fast), "\n")
ok_fast <- (length(g_fast) > 0 || length(d_fast) > 0)
}, silent = TRUE)
# 2) If fast fails, try Selenium and scroll
if (!ok_fast) {
cat("Fast path did not find nodes. Trying Seleniumâ€¦\n")
retry_nav(book_url)
try(remDr$executeScript(
"window.scrollTo(0, document.body.scrollHeight);"), silent = TRUE)
Sys.sleep(1.5)
src <- remDr$getPageSource()[[1]]
html_fast <- read_html(src)
g_sel <- html_elements(html_fast, "[data-testid='genresList'] .Button__labelItem")
d_sel <- html_elements(html_fast, ".EditionDetails .DescListItem")
cat("Selenium path: genres nodes =", length(g_sel),
"| edition items =", length(d_sel), "\n")
}
# 3) Extract using the main function to compare
got <- scrape_book_metadata(book_url, wait_timeout = 12)
print(got)
# 4) Save raw HTML if nothing found
if (save_html && (length(got$genres_vec) == 0 && is.na(got$language))) {
fn <- file.path(getwd(), sprintf("debug_book_%s.html",
gsub("\\W+", "_", basename(book_url))))
writeLines(as.character(html_fast), fn, useBytes = TRUE)
cat("Saved HTML for inspection:", fn, "\n")
}
invisible(got)
}
## Safe wrapper
safe_metadata <- purrr::possibly(
scrape_book_metadata,
otherwise = list(genres_vec = character(),
genres = NA_character_,
language = NA_character_),
quiet = TRUE
)
test_url <- "https://www.goodreads.com/book/show/53138095-a-court-of-silver-flames"
res <- scrape_book_metadata(test_url)
str(res)
knitr::opts_chunk$set(echo = TRUE)
# install.packages(c(
#   "RSelenium",
#   "rvest",
#   "xml2",
#   "dplyr",
#   "purrr",
#   "stringr",
#   "readr",
#   "lubridate",
#   "tidyr",
#   "binman",
#   "wdman"
#   ))
suppressPackageStartupMessages({
library(RSelenium)
library(rvest)
library(xml2)
library(dplyr)
library(purrr)
library(stringr)
library(readr)
library(lubridate)
library(tidyr)
library(jsonlite)
library(tibble)
})
message(">> Starting Selenium (Firefox)...")
# change port number if the Selenium seems to be running even if the session
# was closed
port <- as.integer(4569)
rD <- rsDriver(
browser    = "firefox",
geckover   = "latest", # if this causes issues, use a fixed version: "0.35.0"
phantomver = NULL,
check      = FALSE,
verbose    = FALSE,
port       = port,    # use another free port
)
remDr <- rD$client
remDr$open()
# Safe shutdown even if the script fails later
on.exit({
message("\n>> Closing session and Selenium server...")
safe_onexit(remDr$close())
safe_onexit(rD$server$stop())
}, add = TRUE)
# Navigation helpers
go <- function(u) {
stopifnot(is.character(u), length(u) == 1L, !is.na(u), nzchar(trimws(u)))
remDr$navigate(trimws(u))
}
retry_nav <- function(u, attempts = 3, wait = 2) {
for (i in seq_len(attempts)) {
ok <- try({ go(u); TRUE }, silent = TRUE)
if (isTRUE(ok)) return(invisible(TRUE))
Sys.sleep(wait)
}
stop("Could not navigate to: ", u)
}
login_url <- "https://www.goodreads.com/user/sign_in"
n_books <- 100 #number of books in each page
mybooks_url <- paste0(
"https://www.goodreads.com/review/list?view=table&per_page=",
n_books,
"&shelf=all"
)
retry_nav(login_url)
knitr::opts_chunk$set(echo = TRUE)
if (!file.exists(master_path)) {
stop("Master file not found: ", master_path,
"\nRun the full scraper once before using the updater.")
}
# Master file produced previously. Prefer one that includes `view` (review URL)
# and/or a `review_id` column.
master_path <- "output/goodreads_progress_logs.csv"
# Books-per-page when scraping the shelves table
n_books <- 100
# Which shelves to refresh minimally
# (usually just "read", optionally "currently-reading")
shelves_to_check <- c("read")
if (!file.exists(master_path)) {
stop("Master file not found: ", master_path,
"\nRun the full scraper once before using the updater.")
}
old <- readr::read_csv(master_path, show_col_types = FALSE)
View(df_with_meta)
if (!file.exists(master_path)) {
stop("Master file not found: ", master_path,
"\nRun the full scraper once before using the updater.")
}
old <- readr::read_csv(master_path, show_col_types = FALSE)
extract_review_id <- function(x) {
id <- stringr::str_extract(x %||% "", "(?<=/review/show/)\\d+")
suppressWarnings(as.character(id))
}
if (!"review_id" %in% names(old) && "view" %in% names(old)) {
old <- old %>%
mutate(review_id = extract_review_id(view))
}
need_key <- !"review_id" %in% names(old) || all(is.na(old$review_id))
if (need_key) {
message("Old file lacks review_id. Doing a quick shelf scrape to rebuild keys...")
quick <- scrape_shelf_table("read", max_pages = 50, sleep_sec = 1)
quick_keys <- quick %>%
transmute(
title,
book_url,
view,
review_id = extract_review_id(view)
)
# Join using (book_url, title) as a conservative heuristic key.
# Adjust if you keep a better unique key.
old <- old %>%
left_join(quick_keys, by = c("book_url", "title"))
# %>%
#   mutate(review_id = coalesce(review_id.x, review_id.y)) %>%
#   select(-review_id.x, -review_id.y)
}
if (!"review_id" %in% names(old) || all(is.na(old$review_id))) {
stop("Could not infer review_id for existing rows. ",
"Consider re-exporting master with `view`/`review_id`.")
}
current_tbls <- lapply(shelves_to_check, function(sh) {
message(">> Scraping shelf table: ", sh)
scrape_shelf_table(sh, max_pages = 50, sleep_sec = 1)
})
current <- bind_rows(current_tbls)
current <- current %>%
mutate(review_id = extract_review_id(view)) %>%
filter(!is.na(review_id)) %>%
distinct(review_id, .keep_all = TRUE)
View(current)
old_keys <- old %>% filter(!is.na(review_id)) %>% pull(review_id)
new_ids <- setdiff(current$review_id, old_keys)
new_rows <- current %>% filter(review_id %in% new_ids)
# Detect changed rows: compare a few columns (date_* , my_rating, shelves)
cols_to_compare <- intersect(
c("date_started", "date_finished", "my_rating", "shelves"),
intersect(names(old), names(current))
)
# Wide join current vs old on review_id
comp_tbl <- current %>%
select(review_id, all_of(cols_to_compare)) %>%
left_join(
old %>% select(review_id, all_of(cols_to_compare)),
by = "review_id",
suffix = c("", "_old")
)
# For each row, mark TRUE if ANY of the comparison columns changed
changed_flag <- rep(FALSE, nrow(comp_tbl))
for (col in cols_to_compare) {
new_col <- comp_tbl[[col]]
old_col <- comp_tbl[[paste0(col, "_old")]]
# changed if (not both NA) AND (values differ)
changed_here <- !(is.na(new_col) & is.na(old_col)) & (new_col != old_col)
changed_flag <- changed_flag | (changed_here %in% TRUE)
}
changed_ids <- comp_tbl$review_id[changed_flag]
changed_rows <- current %>%
filter(review_id %in% changed_ids)
# Books likely in progress (no finish date)
in_progress <- current %>%
filter(is.na(date_finished))
to_refresh <- bind_rows(changed_rows, in_progress) %>%
distinct(review_id, .keep_all = TRUE)
targets <- bind_rows(new_rows, to_refresh) %>%
distinct(review_id, .keep_all = TRUE)
message(sprintf("New: %d | Refresh: %d | Total targets: %d",
nrow(new_rows), nrow(to_refresh), nrow(targets)))
View(in_progress)
View(in_progress)
summary(current)
old_keys <- old %>% filter(!is.na(review_id)) %>% pull(review_id)
new_ids <- setdiff(current$review_id, old_keys)
new_rows <- current %>% filter(review_id %in% new_ids)
# Detect changed rows: compare a few columns (date_* , my_rating, shelves)
cols_to_compare <- intersect(
c("date_started", "date_finished", "my_rating", "shelves"),
intersect(names(old), names(current))
)
# Wide join current vs old on review_id
comp_tbl <- current %>%
select(review_id, all_of(cols_to_compare)) %>%
left_join(
old %>% select(review_id, all_of(cols_to_compare)),
by = "review_id",
suffix = c("", "_old")
)
# For each row, mark TRUE if ANY of the comparison columns changed
changed_flag <- rep(FALSE, nrow(comp_tbl))
for (col in cols_to_compare) {
new_col <- comp_tbl[[col]]
old_col <- comp_tbl[[paste0(col, "_old")]]
# changed if (not both NA) AND (values differ)
changed_here <- !(is.na(new_col) & is.na(old_col)) & (new_col != old_col)
changed_flag <- changed_flag | (changed_here %in% TRUE)
}
changed_ids <- comp_tbl$review_id[changed_flag]
changed_rows <- current %>%
filter(review_id %in% changed_ids)
# Books likely in progress (no finish date)
in_progress <- current %>%
filter(
is.na(date_finished),
as.Date(date_added, "%b %d 20%y") < as.Date("Jan 01, 2020", "%b %d 20%y")
)
to_refresh <- bind_rows(changed_rows, in_progress) %>%
distinct(review_id, .keep_all = TRUE)
targets <- bind_rows(new_rows, to_refresh) %>%
distinct(review_id, .keep_all = TRUE)
message(sprintf("New: %d | Refresh: %d | Total targets: %d",
nrow(new_rows), nrow(to_refresh), nrow(targets)))
# check that the scrape worls (it takes a r)
# purrr::walk(head(df_read$book_url, 3), test_book_metadata)
## ONLY FOR DEBUGGING
debug_language <- function(book_url) {
library(xml2); library(rvest); library(stringr)
cat("\nTesting:", book_url, "\n")
html <- tryCatch(read_html(book_url), error = function(e) NULL)
if (is.null(html)) {
retry_nav(book_url); Sys.sleep(1)
src <- remDr$getPageSource()[[1]]
html <- read_html(src)
cat("Used Selenium fallback\n")
}
n_desc_items <- length(rvest::html_elements(
html, xpath = "//dl[contains(@class,'DescList')]//div[contains(@class,'DescListItem')]"
))
n_dt <- length(rvest::html_elements(html, xpath = "//dl[contains(@class,'DescList')]//dt"))
cat("DescList items:", n_desc_items, "| dt in DescList:", n_dt, "\n")
labs <- rvest::html_elements(
html, xpath = "//dl[contains(@class,'DescList')]//dt"
) |>
rvest::html_text2() |>
stringr::str_squish()
if (length(labs)) {
cat("Labels under DescList:", paste(head(labs, 10), collapse = " | "), "\n")
}
# Quick extraction using the same logic as above, condensed
lang_labels <- c("Language","Idioma","LÃ­ngua","Langue","Sprache","Lingua")
lang_node <- rvest::html_element(
html,
xpath = paste0(
"//dl[contains(@class,'DescList')]//div[contains(@class,'DescListItem')]",
"[.//dt[normalize-space() = '",
paste(lang_labels, collapse = "' or normalize-space() = '"),
"']]"
)
)
if (!is.null(lang_node)) {
val <- rvest::html_elements(
lang_node, xpath = ".//dd//*[(@data-testid='contentContainer') or self::dd]"
) |>
rvest::html_text2() |>
stringr::str_squish()
val <- val[nzchar(val)]
cat("Language value:", if (length(val)) val[[1]] else "<none>", "\n")
} else {
cat("Language block not found in DescList. Will rely on generic/legacy fallback.\n")
}
invisible(TRUE)
}
test_url <- "https://www.goodreads.com/book/show/53138095-a-court-of-silver-flames"
res <- scrape_book_metadata(test_url)
str(res)
View(df_read_with_progress)
safe_metadata <- purrr::possibly(
scrape_book_metadata,
otherwise = list(
genres_vec = character(),
genres     = NA_character_,
language   = NA_character_
),
quiet = TRUE
)
message(">> Started scraping metadata (genres + language) â€” this can take a while")
# Optional cache to avoid re-scraping the same URL in a single run
if (!exists(".meta_cache", inherits = FALSE)) {
.meta_cache <- new.env(parent = emptyenv())
}
n <- nrow(df_read_with_progress)
message(paste(
"This is going to take a while, you have",
n_books,
"books. That's around",
n_books * 15 / 60,
"minutes maximum (very rough estimate)."
))
pb <- utils::txtProgressBar(min = 0, max = n, style = 3)
i  <- 0L
df_with_meta <- df_read_with_progress %>%
dplyr::mutate(
.meta = purrr::map(book_url, ~{
i <<- i + 1L
utils::setTxtProgressBar(pb, i)
get_meta_cached(.x)
})
) %>%
dplyr::mutate(
genres_vec = purrr::map(.meta, ~ .x$genres_vec %||% character()),
genres     = purrr::map_chr(.meta, ~ .x$genres     %||% NA_character_),
language   = purrr::map_chr(.meta, ~ .x$language   %||% NA_character_)
) %>%
dplyr::select(-.meta)
close(pb)
message(">> Finished scraping metadata (genres + language)")
View(df_with_meta)
View(new_rows)
View(targets)
safe_metadata <- purrr::possibly(
scrape_book_metadata,
otherwise = list(genres = NA_character_, language = NA_character_),
quiet = TRUE
)
# This is going to take a while
# ðŸ“‰ 15 seconds/book
# ðŸ“‰ From 200 books = 50 seconds â†’ 2â€“10 minutes
print(paste(
"This is going to take a while, you have",
length(targets$book_url),
"books. That's around",
length(targets$book_url) * 15 / 60,
"minutes maximum"
))
meta_list <- map(targets$book_url, safe_metadata)
# Done
meta_df <- tibble(
review_id = targets$review_id,
genres = map_chr(meta_list, ~ .x$genres %||% NA_character_),
language = map_chr(meta_list, ~ .x$language %||% NA_character_)
)
targets <- targets %>% left_join(meta_df, by = "review_id")
View(meta_df)
